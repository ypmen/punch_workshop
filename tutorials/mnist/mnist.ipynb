{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUNCH4NFDI TA5 Workshop (Dresden, 2024) ![PUNCH](punch.png \"PUNCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial guides you through the process of converting a QKeras machine learning model into Vitis HLS code. You'll learn how to apply quantization-aware training (QAT) and model pruning using QKeras to optimize your model. Additionally, the tutorial covers the steps to transform the optimized model into Vitis HLS code using the HLS4ML framework, with a practical example using the MNIST dataset. You can also find the official hls4ml tutorial https://github.com/fastmachinelearning/hls4ml-tutorial.\n",
    "\n",
    "## Requirements\n",
    "- tensorflow/qkeras (build the ML model)\n",
    "- HLS4ML (convert the QKeras model to HLS) https://github.com/fastmachinelearning/hls4ml\n",
    "- Vitis_HLS 2022.2 (complie the HLS code and export the IP)\n",
    "- Vitis/Vivado 2022.2 (generate the xclbin file that can run on Alveo card)\n",
    "- Xilinx Runtime (XRT is a low level communication layer (APIs and drivers) between the host and the card.) https://xilinx.github.io/XRT/2022.2/html/index.html\n",
    "- Development Target Platform (The deployment target platform is the communication layer physically implemented and flashed into the card.) https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/alveo/u55c.html\n",
    "- pynq >3.0.1 (a Jupyter-based framework with Python APIs for using AMD Xilinx Adaptive Computing platforms) https://pynq.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import qkeras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "#disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Vitis HLS path\n",
    "os.environ['PATH'] = '/home/ypmen/Data/Xilinx/Vitis_HLS/2022.2/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST database\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. (from Wiki)\n",
    "\n",
    "![MNIST](mnist.png \"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = (train_images) / 255.0\n",
    "test_images = (test_images) / 255.0\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QKeras model\n",
    "QKeras is a quantization extension to Keras that provides drop-in replacement for some of the Keras layers, especially the ones that creates parameters and activation layers, and perform arithmetic operations, so that we can quickly create a deep quantized version of Keras network. (https://github.com/google/qkeras)\n",
    "\n",
    "![CNN](CNN.jpg \"CNN\")\n",
    "\n",
    "The model is a simple CNN with two convolutional layers. To ensure consistency with the HLS model, both the input and output of each layer are quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from qkeras import *\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "\tQActivation(activation=quantized_bits(8, 2), input_shape=(28, 28, 1)),\n",
    "    QConv2D(8, 3, activation=quantized_bits(8, 2), kernel_quantizer=quantized_bits(8,2,alpha=1), bias_quantizer=quantized_bits(8,2,alpha=1), kernel_initializer='lecun_uniform'),\n",
    "\tQActivation(activation=quantized_relu(8, 2)),\n",
    "    QBatchNormalization(beta_quantizer=quantized_bits(32, 8), gamma_quantizer=quantized_bits(32, 8), mean_quantizer=quantized_bits(32, 8), variance_quantizer=quantized_bits(32, 8)),\n",
    "    QActivation(activation=quantized_bits(8, 2)),\n",
    "\tMaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "    QConv2D(8, 3, activation=quantized_bits(8, 2), kernel_quantizer=quantized_bits(8,2,alpha=1), bias_quantizer=quantized_bits(8,2,alpha=1), kernel_initializer='lecun_uniform'),\n",
    "\tQActivation(activation=quantized_relu(8, 2)),\n",
    "    QBatchNormalization(beta_quantizer=quantized_bits(32, 8), gamma_quantizer=quantized_bits(32, 8), mean_quantizer=quantized_bits(32, 8), variance_quantizer=quantized_bits(32, 8)),\n",
    "    QActivation(activation=quantized_bits(8, 2)),\n",
    "\tMaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
    "\tFlatten(),\n",
    "   \tQDense(10, kernel_quantizer=quantized_bits(8,2,alpha=1), bias_quantizer=quantized_bits(8,2,alpha=1), kernel_initializer='lecun_uniform'),\n",
    "    Softmax()\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# callbacks\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint('chechpoint/test.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels), callbacks=[tensorboard_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(test_images)\n",
    "predicted = np.argmax(pred, axis=1)\n",
    "report = classification_report(test_labels, predicted)\n",
    "\n",
    "print(report)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('mnist.h5')\n",
    "\n",
    "# Load the model\n",
    "# model = qkeras.utils.load_qmodel('mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning model\n",
    "\n",
    "Small weights in the model are set to zero, allowing their corresponding multiplications to be removed during the synthesis of HLS code to HDL. This can reduce the usage of the fpga resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, 0),\n",
    "    'block_size': (1, 1),\n",
    "    'block_pooling_type': 'AVG'\n",
    "}\n",
    "\n",
    "def apply_pruning(layer):\n",
    "  if isinstance(layer, QDense):\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "  elif isinstance(layer, QConv2D):\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "  return layer\n",
    "\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_pruning,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    # Log sparsity and other metrics in Tensorboard.\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)\n",
    "]\n",
    "\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_for_pruning.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = model.predict(test_images)\n",
    "predicted = np.argmax(pred, axis=1)\n",
    "report = classification_report(test_labels, predicted)\n",
    "\n",
    "print(report)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HLS4ML\n",
    "\n",
    "![hls4ml](hls4ML.jpg \"hls4ml\")\n",
    "\n",
    "The QKeras model is converted to HLS using the HLS4ML framework. Configuration settings, such as precision and reuse factor, are specified in a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", 'r') as ymlfile:\n",
    "\tconfig = yaml.safe_load(ymlfile)\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='model_1/hls4ml_prj',\n",
    "                                                       part='xcu55c-fsvh2892-2L-e',\n",
    "                                                       io_type='io_stream',\n",
    "                                                       backend='Vitis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complile the QKeras model to a vitis hls project\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the accuracy between the qkeras and hls\n",
    "\n",
    "labels_keras = np.argmax(model.predict(test_images), 1)\n",
    "labels_hls = np.argmax(hls_model.predict(test_images), 1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Keras  Accuracy: {}\".format(accuracy_score(test_labels, labels_keras)))\n",
    "print(\"hls4ml Accuracy: {}\".format(accuracy_score(test_labels, labels_hls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systhesis the hls code to hdl with Vitis HLS\n",
    "# add \"#pragma HLS INTERFACE mode=ap_ctrl_none port=return\" before build\n",
    "\n",
    "hls_model.build(csim=False, synth=True, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Vitis HLS synthesis report\n",
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vitis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! cd vitis/prj\n",
    "! make all TARGET=hw_emu PLATFORM=xilinx_u55c_gen3x16_xdma_3_202210_1\n",
    "! XCL_EMULATION_MODE=hw_emu ./host -x build_dir.hw_emu.xilinx_u55c_gen3x16_xdma_3_202210_1/krnl_ai.xclbin\n",
    "! make all TARGET=hw PLATFORM=xilinx_u55c_gen3x16_xdma_3_202210_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on the Alveo U55C card with PYNQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pynq\n",
    "import numpy as np\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = pynq.Overlay('vitis/prj/build_dir.hw.xilinx_u55c_gen3x16_xdma_3_202210_1/krnl_ai.xclbin')\n",
    "\n",
    "krnl_mm2s = ol.krnl_mm2s_1\n",
    "krnl_s2mm = ol.krnl_s2mm_1\n",
    "\n",
    "mm2s_in = pynq.allocate((28*28,), dtype='int8', target=ol.HBM0)\n",
    "s2mm_out = pynq.allocate((10,), dtype='int16', target=ol.HBM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm2s_in[:] = np.fromfile('/homes/ypmen/alveo/punch/python/input_2.dat', dtype='int8')\n",
    "mm2s_in.sync_to_device()\n",
    "sleep(3)\n",
    "krnl_mm2s.start(mm2s_in, 28*28)\n",
    "krnl_s2mm.start(s2mm_out, 1)\n",
    "s2mm_out.sync_from_device()\n",
    "sleep(3)\n",
    "print(s2mm_out/1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls4ml-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
